Neural networks are mathematical models that use learning algorithms inspired by the brain to store information. Since neural networks are used in machines, they are collectively called an ‘artificial neural network.’
They are used to process complex data and non linear separable data and make predictions by learning from examples.
Neural networks can handle nonlinear algorithms, making them suitable for a wide range of tasks in machine learning.
Components of a Neural Network :Neurons (Nodes),Layers,Weights and Biases,Activation Functions.
Working Mechanism: Forward and Backward Propagation.
Forward Propagation = Z=WX+b  A=G(Z).
The gradients are propagated backward through the network TO CALCULATE Backward Propagation.
Loss Function: Measures the discrepancy between predicted and actual outputs during training.we use  cross entropy function in this file.
Accuracy: Measures the percentage of correctly classified instances in the test dataset.







Principal Component Analysis (PCA) is a dimensionality reduction technique that aims to find the orthogonal axes (principal components) that capture the maximum variance in the data. It achieves this by decomposing the covariance matrix of the standardized data into its eigenvectors and eigenvalues. Mathematically, PCA involves finding the eigenvectors V and eigenvalues λ of the covariance matrix C, The principal components are then formed by selecting the top k eigenvectors corresponding to the largest eigenvalues. Finally, the original data is transformed into the new coordinate system spanned by these principal components by projecting it onto them using the equation Y=XV, where Y is the transformed data matrix and V is the matrix containing the selected eigenvectors.




